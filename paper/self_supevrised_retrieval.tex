\documentclass[10pt,journal,compsoc,twoside]{IEEEtran}

\usepackage[OT1]{fontenc} 
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[backref=true]{biblatex}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{stfloats}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{tabu}
\usepackage[usenames, dvipsnames]{color}

\newcommand{\etal}{\textit{et al}. }
\newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}. }
\newcommand{\lagr}{\mathcal{L}}
\newcommand{\todo}{{\color{red}\textbf{TODO}}}

\newtheorem{theorem}{Theorem}

\addbibresource{bibliography.bib}

\begin{document}
	
	\title{Video Retrieval Using Self-Supervised Deep Representations}
	
	\author{Max Ehrlich, Peng Zhao, Xiawu Sun, Bhargavi Patel}
	
	\markboth{CS828J}{University of Maryland}
	
	\IEEEtitleabstractindextext{%
		\begin{abstract}
			 In the video retrieval problem, we seek to find, in an automated way, videos that have similar semantic or visual properties. Traditional techniques project videos into a high dimensional space using feature extraction and finding nearest-neighbors in the feature space. Recent work has focused on using deep neural-networks to learn the feature space, which requires a large dataset of labeled data. We propose a novel method for generating video retrieval representations based on self-supversied deep networks. Our method combines two self-supervised representations, Odd-One-Out (O3N), and Temporal Smoothness, which allows the network to learn without labeled data. We show that combining the O3N loss and the Temporal Smoothness loss leads to increased accuracy over using the O3N loss alone. We further provide qualitative results for the retrieval properties of our representation using both simple k nearest neighbors search and hamming similarity from locality sensitive hashes. Our results show that the network is able to retrieve videos with similar motion types.
		\end{abstract}
	}
	
	\maketitle
	
	\IEEEdisplaynontitleabstractindextext
	
	\IEEEpeerreviewmaketitle
	
	\input{sections/introduction}
    
    \input{sections/relatedwork}
    
    \input{sections/o3n}
    
    \input{sections/temporalsmoothness}
    
    \input{sections/network}
    
    \input{sections/hashing}
    
    \input{sections/results}
    
    \input{sections/conclusion}
    
	{\small
		\printbibliography
	}
	
\end{document}


