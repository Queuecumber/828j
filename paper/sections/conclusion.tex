\section{Conclusion and Future Work}\label{sec:conclusion}

We have shown a new convolutional neural network architecture for video retrieval. Our network generates an embedding space for videos that effectively captures similarity in motion types. We showed qualitative results to demonstrate this as well as quantitative benchmarks of the working system that uses locality sensitive hashing to improve performance.  We further show the effectiveness of our method by using our weights as an initialization for the action recognition task and fine tuning, which leads to an improvement over using the O3N loss alone.

Future work should focus on capture other aspects of videos that could improve retrieval. One apparent missing piece from our qualitative results was any kind of color space matching. For example, the returned videos were not penalized by being too far apart in overall color palette. This is something that could be easily learned by a self-supervised signal and incorporated as a third loss. Additionally, more concrete semantic information could be included by training for the action recognition task, though this requires a labeled dataset. Finally, since the loss functions are entirely self-supervised, a deeper network architecture could be devised and trained on much larger datasets without overfitting. This should improve the overall performance of the network.